---
title: "London Cycle Hires and Weather"
output: html_document
---
London Cycle Hires and Weather
==============================

My goal is to investigate usage of the London cycle hire scheme, and in particular how it varies with the weather.

```{r, echo=FALSE, warning=FALSE, message=FALSE, cache=TRUE}
library(ggplot2)
library(reshape2)
library(dplyr)
library(lubridate)
library(GGally)
library(scales)
```

```{r, echo=FALSE, cache=TRUE}
# First load the weather.

weather <- read.csv('weather.csv')
names(weather) <- c('date', 't.max', 't', 't.min', 'dew.max', 'dew', 'dew.min',
                    'hum.max', 'hum', 'hum.min', 'pres.max', 'pres', 
                    'pres.min', 'vis.max', 'vis', 'vis.min', 'wind.max',
                    'wind', 'gust', 'precip.mm', 'cloud', 'events', 'wind.dir')

# Remove columns that I'm not going to be looking at
weather <- subset(weather, select=c('date', 't.max', 't', 't.min', 'wind.max',
                                    'wind', 'gust', 'precip.mm', 'events'))

weather$date <- as.Date(weather$date)

# The events are Fog, Rain, Snow, and Thunderstorm. Only rain and fog have 
# enough days to be worth looking at (even fog is marginal, at 36).
weather$rain <- grepl('Rain', weather$events)
weather$fog <- grepl('Fog', weather$events)

# I'm going to need times with no dates attached. The way to do that seems to
# be to have datetimes with a constant date.
#   This can parse numbers (seconds into the day), or strings in the form 
# 'HHMM', or POSIXcts.
as.time <- function (ct) {
  if (is.character(ct))
    return(as.time(as.POSIXct(ct, format='%H%M', tz='GMT')))
  return(as.POSIXct(as.numeric(ct) %% 86400, origin = "1970-01-01", tz='GMT'))
}

# It's easier to wrangle the bike files separately and then put them together
# than to put them together before wrangling, because of the prev.* and d.*
# columns.

read.bikes <- function (fname) {
  bikes <- read.csv(fname)

  # Remove columns we won't be looking at. id/lat/long are constant within each
  # file. The others are constant over the whole dataset.
  bikes$id <- NULL
  bikes$lat <- NULL
  bikes$long <- NULL
  bikes$installed <- NULL
  bikes$locked <- NULL
  bikes$temporary <- NULL

  # Rename and restructure
  names(bikes) <- c('updated', 'name', 'num.bikes', 'num.spaces')
  bikes$updated <- as.POSIXct(strptime(bikes$updated, "%Y-%m-%dT%H:%M:%S%z", 
                                       tz='GMT'))
  
  # Add columns derived from existing ones.
  bikes$updated.time <- as.time(bikes$updated)
  bikes$updated.date <- as.Date(bikes$updated)
  bikes$num.docks <- with(bikes, num.bikes+num.spaces)
  bikes$weekday <- wday(bikes$updated.date, label=T)
  bikes$is.weekday <- wday(bikes$updated.date) %in% seq(2,6)

  # Add columns for prev and diff between updates. I feel like there should be
  # a better way to do this, but I haven't found it. (Time series don't seem to
  # work very well.) Only add ones I actually use.
  bikes$prev.updated <- c(as.POSIXct(NA), head(bikes$updated, -1))
  attributes(bikes$prev.updated)$tzone <- 'GMT'
  bikes$prev.updated.time <- as.time(bikes$prev.updated)
  bikes$prev.updated.date <- as.Date(bikes$prev.updated)
  bikes$prev.num.bikes <- c(NA, head(bikes$num.bikes, -1))
  bikes$prev.num.docks <- c(NA, head(bikes$num.docks, -1))

  bikes$d.updated <- with(bikes, as.numeric(updated - prev.updated))
  bikes$d.num.bikes <- with(bikes, num.bikes - prev.num.bikes)
  
  return(bikes)
}

bikes <- rbind(read.bikes('bikes-sp.csv'),
               read.bikes('bikes-hh.csv'),
               read.bikes('bikes-bp.csv'),
               read.bikes('bikes-es.csv'))

# The subset is justified later on.
bikes.all <- merge(x=bikes, y=weather, by.x='updated.date', by.y='date')
bikes <- subset(bikes.all, d.updated <= 15)
```

```{r, echo=FALSE, cache=TRUE}
# I also want to look at the number of bikes available at specific times. Since 
# I only have snapshots, I'm going to take the first observation after that 
# time on any given day. Here's a function that lets me do that. It takes a 
# time in formats accepted by as.time, and returns a boolean vector to select 
# the appropriate entries from bikes.
at.time <- function(time) {
  time <- as.time(time)
  with(bikes, ifelse(prev.updated.date == updated.date, 
                     prev.updated.time < time & time <= updated.time,
                     prev.updated.time < time | time <= updated.time))
}
```

## Univariate Plots Section

Temperature:

```{r, echo=FALSE, cache=TRUE}
temps <- melt(weather, id.vars='date', measure.vars=c('t.min','t','t.max'))
ggplot(temps, aes(x=variable, y=value)) + geom_boxplot()
ggplot(temps, aes(x=value, fill=variable)) + geom_density(alpha=0.3)
```

Rainfall:

```{r, echo=FALSE, cache=TRUE}
ggplot(weather, aes(x=rain)) + geom_histogram()
ggplot(weather, aes(x=precip.mm)) + geom_histogram(binwidth=0.2)
table(weather$precip.mm)
```

Wind:

```{r, echo=FALSE, warning=FALSE, cache=TRUE}
winds <- melt(weather, id.vars='date',
              measure.vars=c('wind','wind.max','gust'))
ggplot(winds, aes(x=variable, y=value)) + geom_boxplot()
ggplot(winds, aes(x=value, fill=variable)) + geom_density(alpha=0.3)
```

Time between updates:

```{r, cache=TRUE}
ggplot(bikes.all, aes(x=d.updated, y=..count..+1)) + 
  geom_histogram(binwidth=20) +
  scale_y_log10()
```

A few outliers. Let's zoom in on them.

```{r, cache=TRUE}
ggplot(bikes.all[bikes.all$d.updated >= 5000,], aes(x=d.updated)) +
  geom_histogram(binwidth=5)
bikes.all[bikes.all$d.updated >= 5000, c('name', 'prev.updated', 'updated')]
```

and on the lower ones:

```{r, cache=TRUE}
ggplot(bikes.all[bikes.all$d.updated < 5000,], 
       aes(x=d.updated, y=..count..+1)) +
  geom_histogram(binwidth=5) +
  scale_y_log10()

bikes.all[bikes.all$d.updated >= 2000 & bikes.all$d.updated < 5000,
          c('name', 'prev.updated', 'updated')]

ggplot(bikes.all[bikes.all$d.updated < 60,], aes(x=d.updated)) + 
  geom_histogram(binwidth=1)
```

Date and time of update:

```{r, echo=FALSE, cache=TRUE}
ggplot(bikes.all, aes(x=updated.date)) + 
  geom_histogram(binwidth=1) + 
  scale_x_date()

ggplot(bikes.all, aes(x=updated.time)) + 
  geom_histogram(binwidth=600) +
  scale_x_datetime()
```

## Univariate Analysis

The temperature is a little surprising. I didn't expect the graphs to be bimodal. But we would expect the three measures to have similar shapes, and bimodality could be caused by e.g. an abrupt shift between summer and winter.

Although more than half of observations have `rain == TRUE`, more than half of them also have `precip.mm == 0`, which needs more investigation.

There are five instances where the time between updates is approximately ten days. Four of these (one per station) happened when my collection script broke and I failed to realize it. The other occurred when Southampton Place was taken out of service temporarily.

Then there are a several instances where the time between updates is unusually large, on the order of hours or days. It looks like these happened to all stations simultaneously, suggesting problems with either my collection script or the API, rather than problems with individual locations.

But in the vast majority of cases, updates are approximately ten minutes apart. This encourages me to take a subset of the data (`bikes.all` -> `bikes`), eliminating outliers in future graphs.

All times of day are approximately equally represented, which is good. Dates are a lot less uniform, however. Even apart from the ten-day period where my script was broken, many days have significantly fewer updates than typical, and some have none at all.

## Bivariate Plots Section

Rainfall as measured by `precip.mm` versus as measured by `rain`:

```{r, echo=FALSE, cache=TRUE}
ggplot(weather, aes(y=precip.mm, x=rain)) + geom_boxplot()
ggplot(weather, aes(x=precip.mm, fill=rain, color=rain)) + 
  geom_histogram(position='stack', binwidth=0.1)

table(weather$rain, weather$precip.mm != 0)
```

Journeys taken on rainy vs. non-rainy days, for different measures of rain:

```{r, warning=FALSE, cache=TRUE}
ggplot(group_by(bikes, rain) %>% mutate(count=1/length(rain)),
       aes(x=d.num.bikes, y=count)) + 
  geom_bar(stat='identity') +
  scale_x_discrete() +
  facet_wrap(~rain)

group_by(bikes, rain) %>% summarise(mean(abs(d.num.bikes)))

bikes$rain2 <- bikes$precip.mm != 0
ggplot(group_by(bikes, rain2) %>% mutate(count=1/length(rain2)),
       aes(x=d.num.bikes, y=count)) + 
  geom_bar(stat='identity') +
  scale_x_discrete() +
  facet_wrap(~rain2)

group_by(bikes, rain2) %>% summarise(mean(abs(d.num.bikes)))

group_by(bikes, rain, rain2) %>% summarise(mean(abs(d.num.bikes)))
```

And foggy versus non-foggy days:

```{r, warning=FALSE, cache=TRUE}
ggplot(group_by(bikes, fog) %>% mutate(count=1/length(fog)),
       aes(x=d.num.bikes, y=count)) + 
  geom_bar(stat='identity') +
  scale_x_discrete() +
  facet_wrap(~fog)

group_by(bikes, fog) %>% summarise(mean(abs(d.num.bikes)))
```

Journeys by temperature and wind:

```{r, cache=TRUE}
ggplot(bikes, aes(x=t, y=abs(d.num.bikes))) + 
  geom_jitter(alpha=0.05) + stat_smooth()
cor.test(bikes$t, abs(bikes$d.num.bikes))

ggplot(bikes, aes(x=wind, y=abs(d.num.bikes))) + 
  geom_jitter(alpha=0.01) + stat_smooth()
cor.test(bikes$wind, abs(bikes$d.num.bikes))
```

Length of time spent with a given number of active docks:
```{r, echo=FALSE, warning=FALSE, cache=TRUE}
ggplot(bikes, aes(x=prev.num.docks, y=d.updated/60/24)) +
  geom_histogram(stat='identity') +
  facet_wrap(~name)
```

Journeys taken throughout the year:

```{r, cache=TRUE}
ggplot(bikes, aes(x=updated, y=abs(d.num.bikes))) + 
  geom_jitter(alpha=0.05) + stat_smooth()
```

Journeys by weekday:

```{r, warning=FALSE, cache=TRUE}
ggplot(bikes, aes(x=weekday, y=abs(d.num.bikes))) + 
  geom_bar(stat='identity')
ggplot(bikes, aes(x=weekday, y=num.bikes)) + geom_boxplot()

ggplot(bikes, aes(x=num.bikes/num.docks)) + 
  geom_density() + 
  facet_wrap(~weekday)
```

Number of slots available at 0930, when I'm trying to arrive at work:

```{r, cache=TRUE}
ggplot(bikes[at.time('0930') & bikes$is.weekday,] %>% 
         group_by(name) %>% 
         mutate(count=1/length(name)),
       aes(x=num.spaces, y=count)) +
  geom_bar(stat='identity',binwidth=1) +
  facet_wrap(~name)
```

And at 0940, in case I'm running late:

```{r, cache=TRUE}
ggplot(bikes[at.time('0940') & bikes$is.weekday,] %>% 
         group_by(name) %>% 
         mutate(count=1/length(name)),
       aes(x=num.spaces, y=count)) +
  geom_bar(stat='identity',binwidth=1) +
  facet_wrap(~name)
```

## Bivariate Analysis

`rain` and `precip.mm` don't always agree. Sometimes `rain` is false but `precip.mm` is nonzero; and often `rain` is true but `precip.mm` is zero. Neither of those is surprising individually: if `rain` is only counted when the rainfall exceeds a certain threshold, then that threshold could be large (giving false/nonzero) or small (giving true/zero). But the combination suggests that that isn't what's going on, and I don't know what is.

I find `precip.mm` to me more plausible here. I feel like fewer than half of days are rainy. [This website](http://www.london.climatemps.com/) agrees with me, saying that on average, 164 days out of the year are rainy (`rain` - 237, `precip.mm` - 158). Nevertheless, `rain` was capturing something that `precip.mm` didn't, because bike usage responded slightly more to it. This would seem to suggest that days where `rain` is true but `precip.mm` is zero have less bike usage than average; and indeed this is what we see.

Taking `rain` to be our measure, slightly over 70% of observations had no bikes added or removed on rainy days, and slightly under 70% on non-rainy days. The mean absolute difference is about 25% higher on non-rainy days.

On the other hand, fog, wind and temperature make approximately no difference.

It was common for every station to report less than a full complement of docks. Two stations had a full complement less than half the time. This isn't surprising, since a bike reported as defective will be locked in, using up a slot but not being available for hire.

The time of year makes very little difference to the number of rides. There appears to be a slight sinusoidal relationship, but it's very weak. (I didn't do a PMCC test on that because that assumes that any relationship is linear, which we would naively expect not to be the case here, and also doesn't look true from the graph.)

Fewer journeys are taken on weekends. The median number of bikes available doesn't change much throughout the week, but the distribution does. Saturday and Sunday have noticeably different shapes to the others. They have a single peak, while weekdays are somewhat bimodal, with a small peak where the station is full (probably when people are arriving at work).

(Since the stations have different numbers of docks, I did a graph of fullness rather than of number of bikes. The density plot doesn't show peaks exactly at 0 and 1 because of how the window works, but histograms of num.bikes and num.spaces show that that's where they are. It would be difficult to use a histogram for this graph because there's no sensible binwidth.)

If I'm late, I have slightly less chance of finding a docking station, but not much less.

## Multivariate Plots Section

Correlation between #bikes between ticks:

```{r, warning=FALSE, cache=TRUE}
ggplot(bikes, aes(x=num.bikes, y=prev.num.bikes, color=name)) + 
  geom_jitter(alpha=0.05)

cor.test(bikes$num.bikes, bikes$prev.num.bikes)
```

Bikes at any given time:

```{r, warning=FALSE, cache=TRUE}
ggplot(bikes, aes(x=updated.time, y=num.bikes, color=name)) + 
  geom_jitter(alpha=0.05, shape=1) +
  stat_smooth()
```

Bikes depending on rain:

```{r, warning=FALSE, cache=TRUE}
ggplot(bikes, aes(x=updated.time, y=num.bikes, color=rain)) +
  geom_jitter(alpha=0.05, shape=1) +
  stat_smooth()
```


## Multivariate Analysis

There's very strong correlation between the number of bikes at each axis. This is as expected, especially given what we saw about `d.num.bikes` previously. The colors don't show any particular station-dependent trends.

The correlation also looks strong between the number of bikes at each station at any given time. Since they're all close to each other, that's not surprising either. The time is also a big factor, with large numbers of bikes in the stations during office hours, and few numbers in the evening and early morning.

Rain reduces the variance, with fewer bikes during office hours and more outside of them.

## Reformatting 

With the data in the current format, not all the questions we want to ask are easy. For example: how does the number of bikes at one station correlate with another at any given time?

To answer questions like that, we need to be somewhat forgiving with our definition of 'any given time'. Updates don't necessarily happen simultaneously, so we need to bin them together.

I'm going to create bins ten minutes wide, and assign every observation to a bin. Then in each bin, we can ask how many bikes were at each station.

```{r, cache=TRUE}
# Round updated.time to the nearest ten minutes
tmp <- as.numeric(bikes$updated.time) + 300
bikes$updated.time.bin <- as.time(tmp - tmp%%600)
rm(tmp)

# This gives a slightly different result: the latest :?5:00 time between
# updated.time and prev.updated.time, or NA if there isn't one. (Day boundary
# caveat: If the updates were e.g. at 23:50:00 and  00:10:00, then the bin will
# be 23:55 rather than 00:05.)
#   I'm not sure which version is more appropriate, but there's not much in it,
# so I'm using the other one.

# for (time in seq(300, 60*60*24, 600)) {
#   bikes$updated.time.bin <- 
#     ifelse(at.time(as.time(time)), time, bikes$updated.time.bin)
# }
# bikes$updated.time.bin <- as.time(bikes$updated.time.bin)

# Create wide-format data. The station ordering comes from the factor levels of # bikes$name, which comes from the order I originally loaded them.
bikes.wide <- dcast(bikes, updated.date + updated.time.bin ~ name,
                    value.var='num.bikes', fun.aggregate=mean)
names(bikes.wide) <- c('date', 'time', 'sp', 'hh', 'bp', 'es')
bikes.wide$total <- with(bikes.wide, sp + hh + bp + es)
```

Using this, we can check correlation between each station:

```{r, echo=FALSE, warning=FALSE, cache=TRUE}
ggpairs(bikes.wide, columns=3:6,
        lower=list(params=c(alpha=0.05, position='jitter')))
```

Correlations range between 0.7 and 0.757, and the scatter plots all look pretty similar. Does the correlation depend on time? Let's go for 0930, 1800, midnight, and noon.

```{r, warning=FALSE, cache=TRUE}
ggpairs(bikes.wide[bikes.wide$time == as.time('0930'),], columns=3:6,
        lower=list(params=c(alpha=0.05, position='jitter')))

ggpairs(bikes.wide[bikes.wide$time == as.time('1800'),], columns=3:6,
        lower=list(params=c(alpha=0.05, position='jitter')))

ggpairs(bikes.wide[bikes.wide$time == as.time('0000'),], columns=3:6,
        lower=list(params=c(alpha=0.05, position='jitter')))

ggpairs(bikes.wide[bikes.wide$time == as.time('1200'),], columns=3:6,
        lower=list(params=c(alpha=0.05, position='jitter')))

```

The correlations are almost all lower. That surprised me, but I think it's an example of [Simpson's paradox](http://en.wikipedia.org/wiki/Simpson%27s_paradox). I note that the darkest points in the graph are at midnight, with no bikes in any station much of the time. Bikes are periodically moved in vans to account for anticipated demand; I assume that these stations are emptied most nights to prepare for people coming to work in the morning.

We can view a histogram of the total number of bikes available at different times:

```{r, warning=FALSE, cache=TRUE}
ggplot(bikes.wide, aes(x=total)) + 
  geom_histogram(binwidth=1) + 
  facet_wrap(~time)
```

We see heavy leftward skews overnight, with much flatter (but somewhat right-skewed) distributions during office hours, and gradual transitions between the two.

We can also check correlation between times more distant than a single tick. If I check the slots available when I leave the house, can I learn how many will be there when I arrive?

```{r, warning=FALSE, cache=TRUE}
spaces.0900.0930 <- 
  dcast(bikes[bikes$updated.time.bin %in% as.time(c('0900', '0930')),], 
        updated.date + name + rain + is.weekday ~ 
          strftime(updated.time.bin, 'at.%H%M', tz='GMT'),
        value.var='num.spaces', fun.aggregate=mean)

ggplot(spaces.0900.0930, aes(x=at.0900, y=at.0930, color=name)) +
  geom_jitter(alpha=0.2) +
  stat_smooth()

with(spaces.0900.0930, cor.test(at.0900, at.0930))
```

This is good correlation! Does it depend on the rain?

```{r, warning=FALSE, cache=TRUE}
ggplot(spaces.0900.0930, aes(x=at.0900, y=at.0930, color=rain)) +
  geom_jitter() +
  stat_smooth()

with(spaces.0900.0930, cor.test(at.0900[rain], at.0930[rain]))
with(spaces.0900.0930, cor.test(at.0900[!rain], at.0930[!rain]))
```

Not much, if at all.

We can construct a model

```{r, cache=TRUE}
summary(lm(at.0930 ~ at.0900, spaces.0900.0930))
```

with an R^2 of 0.87, which is pretty good. But this isn't the best we can do, because it groups all stations together. Ideally we would create one model per station, with inputs from every station.

```{r, cache=TRUE}
# Construct a data frame with both long and wide properties. For every day for
# every station, we indicate spaces in that station at 0930, and spaces in all
# stations at 0900.
get.station <- function(short, long) {
  tmp <- spaces.0900.0930[spaces.0900.0930$name==long,
                          c('updated.date', 'at.0900')]
  names(tmp)[2] <- short
  return(tmp)
}
spaces.tmp <- merge(spaces.0900.0930, get.station('sp', 'Southampton Place'))
spaces.tmp <- merge(spaces.tmp, get.station('hh', 'High Holborn'))
spaces.tmp <- merge(spaces.tmp, get.station('bp', 'Bury Place'))
spaces.tmp <- merge(spaces.tmp, get.station('es', 'Earnshaw Street'))

summary(lm(at.0930 ~ sp+hh+bp+es,
           spaces.tmp[spaces.tmp$name=='Southampton Place',]))
summary(lm(at.0930 ~ sp+hh+bp+es,
           spaces.tmp[spaces.tmp$name=='High Holborn',]))
summary(lm(at.0930 ~ sp+hh+bp+es,
           spaces.tmp[spaces.tmp$name=='Bury Place',]))
summary(lm(at.0930 ~ sp+hh+bp+es,
           spaces.tmp[spaces.tmp$name=='Earnshaw Street',]))
```

Southampton Place has slightly regressed, but the others have improved slightly. (It's important to note that this doesn't make our model worse for Southampton Place than the aggregate model. The aggregate model was just overconfident on that station.)

## Final plots and summary

### Plot 1

```{r, echo=FALSE, warning=FALSE, cache=TRUE, fig.width=11, fig.height=11}
ggplot(mutate(bikes.wide, timestr=strftime(time, '%H:%M', tz='GMT')),
       aes(x=total)) + 
  geom_histogram(binwidth=1) + 
  facet_wrap(~timestr) +
  xlab('Total number of bikes available') +
  ylab('Frequency') +
  ggtitle('Number of bikes available throughout the day')
```

The total number of bikes available changes gradually throughout the day, with few bikes typically available at night, but often many available during the daytime.

### Plot 2

```{r, echo=FALSE, warning=FALSE, cache=TRUE, fig.width=11, fig.height=11}
ggplot(subset(spaces.0900.0930, is.weekday),
       aes(x=at.0900, y=at.0930, color=name)) +
  geom_jitter(alpha=0.3, position=position_jitter(width=0.25, height=0.25)) +
  stat_smooth(alpha=0.1, linetype='dashed') +
  scale_x_continuous(breaks=seq(0, 22)) +
  scale_y_continuous(breaks=seq(0, 22)) +
  xlab('Spaces available at 09:00') +
  ylab('Spaces available at 09:30') +
  ggtitle('Spaces available during the week at 09:00 and 09:30, by station') +
  geom_line(data=data.frame(x=c(0,21), y=c(0, 21)),
            aes(x=x, y=y),
            color='#555555', linetype='dashed') +
  guides(color=guide_legend(title='Station'))
```

This time around, I restricted the graph to weekdays only. It's rare for the number of stations to go up between 0900 and 0930. All four stations have similar usage patterns.

### Plot 3

```{r, echo=FALSE, warning=FALSE, cache=TRUE, fig.width=11, fig.height=11}
ggplot(bikes, aes(x=updated.time.bin, y=num.bikes, color=name)) +
  geom_line(aes(linetype='1'), stat='summary', fun.y=mean) +
  geom_line(aes(linetype='2'), stat='summary', fun.y=median, alpha=0.5) +
  geom_line(aes(linetype='3'), stat='summary', fun.y=quantile, probs=0.9) +
  geom_line(aes(linetype='3'), stat='summary', fun.y=quantile, probs=0.1) +
  scale_x_datetime(breaks=as.POSIXct(seq(0, 86400, 3600*3),
                                     origin='1970-01-01'),
                   labels=date_format('%H:%M')) +
  xlab('Time') + ylab('Bikes available') +
  ggtitle('Number of bikes available by time and station') +
  scale_linetype_manual(name='Statistic', values=c('solid', 'dashed', 'dotted'),
                        labels=c('Mean', 'Median', '10th/90th percentile')) +
  guides(color=guide_legend(title='Station'))
```

I took advantage of binning to calculate specific summary functions. All stations show similar patterns: at night, there are few bikes available; during office hours, there are almost always some, and the 10-90 percentile range is a lot higher.

## Reflection

I've learned a lot about how to fight ggplot when it doesn't do exactly what I want by default, and in particular about how to shape my data.

I feel like a data frame isn't an ideal structure for the data I have. The fact that I had to create `prev.*` and `d.*` copies of those columns that need it seems suboptimal, ideally I would have wanted to be able to refer directly to offset rows in the data. (For example, there's currently no easy way to ask "what's the difference between the number of bikes now and 30 minutes ago?") But I couldn't find anything that worked better. In particular, time series only allow one data type, so I would have had to fight to use them at all, and I don't know if they would have been any more useful.

My data set itself isn't ideal, particularly in the amount of missing data. Unfortunately, I don't think any better historical bike record data is available. I think I have enough data to trust my conclusions.

In general, it seems that weather doesn't have much impact on bike usage. I checked rain, fog, temperature and wind speed, and only rain made a significant difference. But since the rainfall data seems to be internally inconsistent, I don't know how much we can learn from it. It would be useful to validate it from another source.

On the other hand, we can make pretty good predictions about future bike (and slot) availability just from current availability. An ambitious future project might be a prediction system. A user could specify a station and an arrival time, and the system could tell her how likely it would be that she could find a slot in that station and nearby ones, and suggest an earlier arrival time that would increase that chance.

One thing I didn't examine was [public holidays](http://en.wikipedia.org/wiki/Bank_holiday#List_of_current_holidays_in_the_United_Kingdom.2C_Ireland_and_the_Isle_of_Man). For example, we might ask whether, on plot 2 above, many of the points where spaces were freed up fell on holidays. (We can calculate 128 points above the line, and only 8*4 = 32 of them could be on public holidays, but that's still potentially up to a quarter of them.)
